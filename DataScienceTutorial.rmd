---
title: "Data Science Pipeline Tutorial: Coronavirus Edition"
author: "Everett Brown, Samantha Hilbert, Brendan Cron"
date: "5/18/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#setwd("C:\\Users\\Everett\\CMSC320Final");
#setwd("/Users/ahilbert/Desktop/CMSC320Final")
library(lubridate)
library(tidyverse)
library(dplyr)
library(gtrendsR)
library(rvest)
```

## Introduction

Welcome to the world of data science! This webpage will give you a walkthrough of some of the core concepts. This project is written in the language of R but should be explained in enough detail so someone that has never seen R can easily follow. 
The data we will be monitoring is coronavirus and how it has affected different aspects of our lives. We will look at everthing from dates that different states start quarentine to how many people google searched how to make sourdough bread. Hope you enjoy!

## Data Curation, Parsing, and Management
The first part of data science is collecting data. This can be done manually with surveys but it is much more efficient to look for an existing dataset on the internet. There are many ways that we can get the data, 2 of those ways that we will look at is using a comma seperated value (CSV) file or through webscraping.
Lets first look at how to get data from a CSV. For this example we grabbed a dataset from https://github.com/nytimes/covid-19-data and read it into a dataframe which is how we will manage the data for the rest of the project. This CSV contains data about covid cases for each state

```{r}
#if you want to update data, uncomment the following line
#download.file("https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv","states_data.csv")
raw_corona_state_data <- read_csv(paste(getwd(), "/states_data.csv", sep = ""))
```

Here we are reading another CSV into the dataframe to account for state codes

```{r}
state_codes <- read_csv(paste(getwd(), "/state_codes.csv", sep = "")) %>%
    select(State, Code) %>%
    mutate(Code = paste("US-",Code,sep=""))
```

Our dataframe is not in the best format for us to use right now. There are some things that we should change in order to handle the data better. We are going to mutate the dates in the table to have a datetime format and filter all of the state codes so they have state names instead.

```{r}
corona_state_data_formatted <- raw_corona_state_data %>%
    mutate(date = ymd(date)) %>%
    select(state, date, cases, deaths) %>%
    filter(state %in% state_codes$State)
```

## Webscraping
Another way that data scientists can grab data is from websites. Unfortunately, this data does not always come in a nice format like CSV files do so we have to fuss with the data more. For this example we are using data from the Wikipedia page https://en.wikipedia.org/wiki/U.S._state_and_local_government_response_to_the_COVID-19_pandemic which has information about different states reactions to the coronavirus pandemic. We are especially curious about the dates that different states declared a stay at home order and how that descision has affected different aspects of our lives as well as how it affected the spread of coronavirus.
Here we simply read the html table from the webpage and insert that data into another dataframe
```{r}
url <- "https://en.wikipedia.org/wiki/U.S._state_and_local_government_response_to_the_COVID-19_pandemic"
raw_lockdown_data <-  url %>% read_html() %>%
    html_node(".wikitable") %>%
    html_table(fill = TRUE) 
```
Just like above, the data is not in the best format. We are going to tidy up the dataframe in order for us to use it much easier in the future. Here we do that by organizing the table first by state and then we will make sure the dates are in the correct format.
```{r}
lockdown_data <- raw_lockdown_data %>%
    setNames(make.names(names(.), unique = TRUE)) %>% 
    select( 1,4) %>%
    slice(2:n())
names(lockdown_data) <- c("state", "lockdown_start")
lockdown_data <- lockdown_data %>%
    mutate(lockdown_start = ymd(paste("2020",lockdown_start))) %>%
    filter(state %in% state_codes$State)
```
##Joining dataframes
One of the most important concepts of data science is joining dataframes. When we want to compare data from 2 different areas usually it is difficult to find a CSV that already has data on both datasets. What we do instead is find 2 dataframes with similar attributes and perform a join on the dataframes effectively merging the data. For this example we will merge together the state data we got from the CSV and the lockdown data that we scraped from Wikipedia. The attribute that we will join on is the state.
```{r}
full_state_data <- corona_state_data_formatted %>%
    left_join(lockdown_data, by = "state") %>%
    mutate(in_lockdown = ifelse(is.na(lockdown_start), F, ifelse(date >= lockdown_start, T,F)))
```
Another area that we want to look at for our project is google trends. This will give us search records for what people searched during which periods of time. This can give us insights on people's behaviors and also interests. Luckily google provides easy methods to download a CSV with this data but to make our code less repetitive, below we made a function that takes in a term and returns a dataframe containing the data for each term and the popularity of that term for each state. 
```{r}
get_term <- function(term){
  all_trends <- gtrends(keyword = term, geo = state_codes$Code[1], time = "today 3-m", onlyInterest = T)[[1]]
    all_trends <- all_trends %>%
       select(date, hits) %>%
       mutate(state = state_codes$State[1]) %>%
       mutate(date = ymd(date))
  
  names(all_trends)[2] = paste(term,"_hits", sep = "")
  
  for(i in 1:50){
    trend_df <- gtrends(keyword = term, geo = state_codes$Code[i], time = "today 3-m", onlyInterest = T)[[1]]
    
    trend_df <- trend_df %>%
       select(date, hits) %>%
       mutate(state = state_codes$State[i]) %>%
       mutate(date = ymd(date))
    names(trend_df)[2] = paste(term,"_hits", sep = "")
  
    all_trends <- rbind(trend_df, all_trends)
  }
  
  return(all_trends)
}
```
Note: only data from last 90 days is included
Now we use this function above to create new dataframes with all of the data from the trends. We noticed pulling the data from google takes a while, expect 20-30 seconds per term. Additionally sometimes it just fails, the gtrends() doesn't always work. The two trends that we will specifically look at are coronavirus and sourdough.
```{r, eval=FALSE}
#coronavirus_term <- get_term("coronavirus")
#sourdough_term <- get_term("sourdough")
#bread_term <- get_term("bread")
#ticketmaster_term <- get_term("ticketmaster")
#zoom_term <- get_term("zoom")
```
Unfortunately this code chunk above didn't seem to be the most reliable, not due to our code, but sometimes google trends got flooded with requests and failed. Our solution to this was run each term but ultimately save the data as a csv and just read it from that. The above code technically works however it can be inconsistent
```{r, eval=FALSE}
write.csv(coronavirus_term,"coronavirus_term.csv",row.names = FALSE)
write.csv(sourdough_term,"sourdough_term.csv",row.names = FALSE)
write.csv(bread_term,"bread_term.csv",row.names = FALSE)
write.csv(ticketmaster_term,"ticketmaster_term.csv",row.names = FALSE)
write.csv(zoom_term,"zoom_term.csv",row.names = FALSE)
```
Here is the code from when we actually run the code in order to be more consistent. Remember this code is completely optional we just did it so our code would run faster on average case
```{r}
coronavirus_term <- read_csv(paste(getwd(), "/coronavirus_term.csv", sep = ""))
sourdough_term <- read_csv(paste(getwd(), "/sourdough_term.csv", sep = ""))
bread_term <- read_csv(paste(getwd(), "/bread_term.csv", sep = ""))
ticketmaster_term <- read_csv(paste(getwd(), "/ticketmaster_term.csv", sep = ""))
zoom_term <- read_csv(paste(getwd(), "/zoom_term.csv", sep = ""))
```
Now that we have all of the term dataframes we can join them with our other data frame to get a full summary of any data that may seem interesting to us to use for analysis!
```{r}
corona_state_data <- full_state_data
corona_state_data <-  corona_state_data %>% left_join(coronavirus_term, by = c("state","date"))
corona_state_data <-  corona_state_data %>% left_join(sourdough_term, by = c("state","date"))
corona_state_data <-  corona_state_data %>% left_join(bread_term, by = c("state","date"))
corona_state_data <-  corona_state_data %>% left_join(ticketmaster_term, by = c("state","date"))
corona_state_data <-  corona_state_data %>% left_join(zoom_term, by = c("state","date"))
corona_state_data
```
One last thing we will do to our dataframe is add some normalized data which we will use later on in the project. Normalizing data is just a fancy way of saying that we're changing the range of the data but keeping the distribution shape. In this case we are putting the cases and deaths on a zero to 100 scale so we can more easily reference them in the future in comparison to trends which also exist on a zero to 100 scale. You can visit this youtube link to learn more about data normalization in R: https://www.youtube.com/watch?v=hot0Wxt9lpQ
```{r}
corona_state_data <- corona_state_data %>%
  group_by(state) %>%
  mutate(max_cases = max(cases)) %>%
  mutate(max_deaths = max(deaths)) %>%
  ungroup() %>%
  mutate(percent_cases = 100*cases/max_cases) %>%
  mutate(percent_deaths = 100*deaths/max_deaths) 
  
```
## Exploratory data analysis
Great! Now we have all of our data! what can we do with it? One thing that we can do is visualize it in order to better understand the bigger trends of the data. The first thing we can look at is each state's covid cases and how the infection rate changed after a stay at home order was put into place.
Since we are looking at a potentially exponential graph, instead of showing the total number of cases we are showing the log of the number of cases. This makes the graph easier to read
```{r}
corona_state_data %>%
  mutate(log_cases = log2(cases)) %>%
  ggplot(aes(x = date, y = log_cases,color = in_lockdown, group=state)) + 
  geom_line()
```
As you can see the graph shows a huge boom in cases from march first to late march, but once states started enforcing a stay at home order, the infection rates dropped mostly, however this may not be statistically signiicant because the states which didnt enforce it also had similar trends. These could be attributed to people choosing to stay at home because of the news or other factors as well.
An interesting thing to take away from this graph is noticing when people's daily lives started changing. It was around mid march. Let's look at some google trends to see how this change affected their searches.
##Google Trends Representation
To avoid data clutter lets just search the trends for Maryland instead of every state individually, however you'll find the trends all look similar. First, let's model the search rates for "coronavirus".
```{r}
corona_state_data %>%
    filter(state=="Maryland") %>%
    ggplot(mapping=aes(x=date, y=coronavirus_hits, group=state)) +
    geom_line()
```
As you can see the trends peaked mid march when the pandemic was getting scarier and scarier but slowly dropped down as a result of people either losing interest or more likely it became old news to them and went on searching whatever else was new to them.
Let's look at a more severe case. When coronavirus hit, people were much less likely to go out to a packed movie theater. Lets look a the trends for one particular site, Ticketmaster. 
```{r}
corona_state_data %>%
    filter(state=="Maryland") %>%
    ggplot(mapping=aes(x=date, y=ticketmaster_hits, group=state)) +
    geom_line()
```
As you can see ticket master was searched very highly before mid march when the spread was minimal but it dropped sharply which makes sense because why buy a ticket if you can't go to the theater?
Finally to account for all the students and workers working from home, let's look up the term "zoom".
```{r}
corona_state_data %>%
    filter(state=="Maryland") %>%
    ggplot(mapping=aes(x=date, y=zoom_hits, group=state)) +
    geom_line()
```
As you can see this chart gradually grew representing the increasing number of people trying to use the software and peaked in april when the quarentine started. These searches gradually became less frequent over time as people figured out how the application worked. There are also periodic dips in zoom searches possibly indicating weekends when people aren't working.
##Death Rates
One common source of confusion is death rates in america. This can be attributed to low testing. Let's look at the data anyways but remeber to take all of this with a grain of salt as we are only looking at broad trends and are less concerned with individual numbers.
Let's also define death rate to be the number of deaths divided by the number of infected or number of current cases.
First lets make a new dataframe with just the data that we need
```{r}
cases_df <- corona_state_data %>%
  select(state, date, cases, deaths) %>%
  mutate(death_rate = ((1.0*deaths)/cases))
cases_df
```
Now lets look at the state death rates over time using ggplot.
```{r}
cases_df %>%
  ggplot(aes(x= date, y=death_rate, group=factor(state))) +
  geom_line()
```
There is a couple main takeaways from this chart. First there are clearly outliers. Those spikes in March were likely caused by the shortage of available tests so the few people that died severly affected the percentage. If you were to do advanced analysis you would likely filter out the data points when the total cases were under 100.
The second main takeaway is that the deathrates are constantly rising from april 1st to May 1st but have since leveled out. This can be due to the deaths "lagging" behind the positive test count as most people don't die immediately after being tested. We cannot be certain of the exact reason as it may be more closely linked to the number of tests provided at the time.
The last alarming point is some states are now sitting at a 10 percent deathrate. Well, this goes back to my disclaimer from the beginning. While the coronavirus is something that is extremely severe and should not be taken lightly, many people will contract the disease without getting tested and people typically only get tested when they show severe symptoms and require hospitalization.This may mean that there are a lot more cases of covid than the chart indicates bringing down the percentage. Needless to say this information should be taken with appropriate skepticism.
##Cross analysis
Not that we have viewed and interpreted different data lets combine two different factors to see how they relate. For example, here is a comparison of searches for "sourdough", and the number of cases of coronavirus in NY which we normalized earlier
```{r}
corona_state_data %>%
    filter(state == "New York") %>%
    ggplot(aes(x = date)) + 
    geom_line(aes(y = percent_cases, col = "Percent of max cases")) + 
    geom_line(aes(y = sourdough_hits, col = "Searches for sourdough"))
```
This is interesting because the number of cases appears to directly affect the rise in people's interests over sourdough bread possibly caused by stay at home isolation. You can see that this trend is slowly phasing out as people find other hobbies.
## Hypothesis Testing
In the above section we looked at a bunch of different data that we have gathered and visualized it. After each one we made some comments on it why it makes sense that the data looked the way it did. However, we didn't prove anything, we just made guesses. Luckily data science gives us tools that can help us make hypotheses for why something happened and can tell us if our hypothesis is correct or not (please dont confuse correlation with causation however! Still be skeptical: https://www.tylervigen.com/spurious-correlations)
One way to do a hypothesis test is attempting to reject a null hypothesis. This seems very confusing but is actually more simple once explained. Say your friend flipped a coin 10 times and it got 7 heads and 3 tails. You would not be skeptical of a rigged coin because of the small sample size. Even though the coin came up heads 70% of the time there was a chance that this was just random and the next 10 flips would come up 50-50. The null hypothesis represents your theory that the coin just had a "lucky chance". However, if your friend flipped a coin 1000 times and there were 700 heads and 300 tails you would become much more skeptical because even though the coin has the same flip rate, theres a much smaller chance that the rate was achieved on accident and the coin may be rigged.
In data science we use a confidence interval. This represents the range which we are confident that our data was just a random fluctuation. However we say if the data lies outside of the confidence interval it must be caused by another factor and we can reject the null hypothesis.
For our example we are going to see if more people searched the word "bread" after being quarentined. The null hypothesis is that the higher number of searches was due to a random fluctuation and not correlated to the stay at home orders. In order to test our hypothesis first we are going to represent the data visually. For each data point we compare that data to if that individual state is in lockdown then we move the datapoints to the corresponding group. We represented the data as a box plot so you could easily see the mean and distribution of the trend.
```{r}
corona_state_data %>% 
    filter(!is.na(bread_hits)) %>%
    ggplot(mapping=aes(x=in_lockdown, y=bread_hits)) +
    geom_boxplot()
```
As you can see the number of searches for bread is clearly higher after entering lockdown. In order to reject the null hypothesis that this relationship is simply due to chance, we need to run a hypothesis test. First we will find the mean and standard distribution of each dataset. Those terms describe the shape of the distribution. For more information about the statistics look here: https://www.thoughtco.com/what-is-statistics-3126367
```{r}
summaries <- corona_state_data %>%
  select(state, date, in_lockdown, bread_hits) %>%
  filter(!is.na(bread_hits)) %>%
  group_by(in_lockdown) %>%
  summarize(mean_bread_hits = mean(bread_hits),
            sd_bread_hits = sd(bread_hits))
summaries
```
We have the distribution data, now we need to run what we call a t-test on the data. Without getting into too mcuh detail (I encourage you to look up the process) it takes your distribution and returns a p-value which represents the probability that your data happened by chance. Interpreting that p-value can be arbitrary so for this example we will say if the p-value is under 0.05 we will be able to reject the null hypothesis demonstrating that there exists a correlation between the data.
```{r}
bread_hits_t_test <- t.test(bread_hits ~ in_lockdown, data = corona_state_data)
print(paste("p-value = ", bread_hits_t_test$p.value))
```
We ended up with an extremely small p-value. Therefore we can reject the null hypothesis! This means by some force the coronavirus lockdown was related to the amount of people googling bread. Exciting stuff! However, do not confuse the fancy graphs and functions with proof of causation. Hypothesis testing can only show correlation.
## Machine Learning
Before training our model and using machine learning, let's talk about what those terms actually mean.

All of the data we have gathered is stored in a dataframe, which is basically just a giant fancy excel spreadsheet. 
Each row in this table represents an entity. In our dataframe, an entity is defined by a state and a date. 
Each entity is unique, for example there is only one entity for Wyoming on February 27th. Each entity also stores other data, called
attributes. Examples of attributes for our dataset would be the number of cases on a day, number of hits for a given search term, 
and wether or not a state is in lockdown. Each attribute is stored in a column. By storing the data like this, it is easy to manipulate the data and answer questions. The following shows the beginning of this dataframe:

```{r}
corona_state_data %>% slice(2000:20005)
```

Lets briefly talk about what some of these terms represent. The state and date column define the entity. This pair can be used to 
uniquely identify a row in the dataframe. The number of cases and deaths are stored in cases and deaths, respectively. 
lockdown_start gives the date lockdown started, or NA if the state never went into lockdown. The in_lockdown column has a value of TRUE if the state 
is in lockdown on that date, and false otherwise. The columns that end in _hits represent the google search popularity for a particular term.
The Sourdough_hits column shows the popularity of the term sourdough on a scale from 0 to 100. 0 means that there was not enough data, and 
100 means that the poplarity peaked in the state on that date. Other values represent the percent of peak popularity. So a value of 50
means that a term is half as popular on the given date as its peak popularity. This scale of 0-100 for each state allows us to compare them,
even if New York had 10 times as much search traffic as North Dakota, both show hits data on a scale from 0-100. 
Similarly, percent_cases is a number between 0 and 100. It represents the number of cases
as a percentage of peak cases in that state.
For example, the total amount of cases washingtown has had is about 19,500. On April 21st, 
Washington had about 12,300 cases. The percentage of total cases seen for Washington on April 21st would be 
100*12,300/19,500, yielding about 63. 63 will be the value of percent_cases for this state/date pair.


For a given date/state pair, What if we covered up the percent_cases attribute and couldn't see it? Could we guess a value for it based on 
the values of all the other attributes? This is the problem a model attempts to solve. It looks at the attriubtesthat we know for an entity , does 
some math behind the scenes, and spits out a predict for the value we don't know. Having this model is useful, because if there is data we don't 
have, we can predict it. This is all well and good, but how do we get a model
that accurately predicts the missing attributes? The answer, is machine learning. Machine learning 
takes in huge datasets, crunches the numbers, and creates a model.

How does machien learning work? There is a wide variety of algorithms that use machine learning to create models, but
the one we will be using is linear regression. It is easy to use but still has predictive power. Given an oucome we want to predict,
we tell it which attributes to look at. We also feed it training data, which is the data we already have values for.
It converts all these attributes into numbers, then multiplies them by constants 
and with each other. It adds and subtracts these numbers and creates a prediction. Luckily for us, these constants are found behind the scenes. If you want to learn more about it here's a good resource: https://www.sas.com/en_us/insights/analytics/machine-learning.html
However, taking some of the functions that we will be using as a black box, we will show you how to harness some of the power that comes with machine learning.

Okay, enough background stuff, let's get predicting!

The google trends data only goes back 3 months, so we will be discluding data from before that time. 
We do that by filtering that dataset and only keeping entities that have a date after February 25th
```{r}
data_for_model <- corona_state_data %>%
  filter(date > "2020-2-25")
```


Before we create our models, we are first going to create
functions to help evaluate these models. The first one will help 
visualize the model and help us see what it does, and the second will 
give a number that tells how well a model performs.



Below is a function that makes a graph of the predicted value for percent_cases 
based on the model provided and the actual value of percent_cases over time.
First, it generates a vector(a list of numbers) of predictions, using the
predict function. This function takes a model and data, and spits out what the model 
predicts for each entity. 

Next, these predictions are combined with the data and put into a dataframe called corona_state_data_with_predictions.
Using the data in this dataframe, the function plots the predicted case amount and the actual case amount for the specified state
[keep going here]
```{r}

plot_prediction <- function(model, given_state){
  predictions <- model %>% predict(data_for_model)
  
  corona_state_data_with_predictions <- data_for_model %>%
    mutate(prediction = predictions[row_number()])
  
  corona_state_data_with_predictions %>%
    filter(state == given_state) %>%
    ggplot(aes(x = date)) + 
    geom_line(aes(y = prediction, col = "Predicted Cases"))  +
    geom_line(aes(y = percent_cases, col = "Cases")) + 
    ggtitle(given_state)
}

#}
```

In order to evaluate how well a model performs, we need a mathematical way to evaluate it.
If we didn't, we would have to look at the plot for each of the 50 states and guess about how well it did for each of them.
That is too difficult for a person! But computers can perform millions of calculations per second, so they are well suited to evaluate
these models. This function calculates the difference between the prediction and the real value of cases. Then it squares this number. It
does this for every entity in the dataframe and takes the average. Then it takes the square root to put the value back into meaningful units.
The scoring is like golf, a low score is better than a high score. 0 is a perfect score. This metric is called the Root Mean Square Error (RMSE)
```{r}

get_RMSE <- function(model){
  
  predictions <- model %>% predict(data_for_model)
  return(Metrics::rmse(data_for_model$percent_cases, predictions))
}

```

Now we have all the tools to create our first model! First we will see how well we can
predict cases based on just the date. The lm function creates a model for us. 
We will plot the predictions for this model in New York, Maryland, Arizona, and Washington to show how it performs.
```{r}
date_model <- lm(percent_cases ~date, data=data_for_model)

plot_prediction(date_model, "New York")
plot_prediction(date_model, "Maryland")
plot_prediction(date_model, "Arizona")
plot_prediction(date_model, "Washington")

print(paste("RMSE: ", get_RMSE(date_model)))

```
A few things to note from the above model, it is the same prediction for each state. This is because 
the only thing it is basing its prediction off of is the date. The RMSE is relatively low, with a value of 12.9 
That means that on average, this model is off by 12.9.










Next we will create a model based on the search popularity of "sourdough". Is that all it takes to predict the amount of coronavirus in a region
```{r}
sour_model <- lm(percent_cases ~sourdough_hits, data=data_for_model)

plot_prediction(sour_model, "New York")
plot_prediction(sour_model, "Maryland")
plot_prediction(sour_model, "Arizona")
plot_prediction(sour_model, "Washington")

print(paste("RMSE: ", get_RMSE(sour_model)))

```
Unfortunately, it turns out this model isn't very good. The RMSE is about 32,
which means the prediction is off by 32ish on average. Also, by looking at the graphs, 
the prediction is way off from the real data






Now lets make a model that looks at both date and sourdough hit rates. Will it perform better than just date alone?
```{r}
sour_date_model <- lm(percent_cases ~sourdough_hits*date, data=data_for_model)

plot_prediction(sour_date_model, "New York")
plot_prediction(sour_date_model, "Maryland")
plot_prediction(sour_date_model, "Arizona")
plot_prediction(sour_date_model, "Washington")

print(paste("RMSE: ", get_RMSE(sour_date_model)))

```
The answer is yes! This model is slightly better than the others, with a RMSE of 11.9. This value is about 1 lower than for the model based on dates.




Finally, this is a model that tries to predict cases based on wether or not the state is in lockdown as well as the date. 
```{r}
lockdown_date_model <- lm(percent_cases ~in_lockdown*date, data=data_for_model)

plot_prediction(lockdown_date_model, "New York")
plot_prediction(lockdown_date_model, "Maryland")
plot_prediction(lockdown_date_model, "Arizona")
plot_prediction(lockdown_date_model, "Washington")

print(paste("RMSE: ", get_RMSE(lockdown_date_model)))

```
This is the best model we've made so far, with a RMSE of 11.7. 


Overall, these models are okay. Remeber, it was trained on data from the whole country. Each state has a very different picture, so it is 
difficult to predict cases based on all states and have it perform extremely well on one state. However, it is interesting that including 
sourdough searches in google improved our models!

##Conclusion
We hope you learned something about data science and maybe something about our changing world. One of the main takeaways for this project is you the datascientist have to be creative and adaptable. It is not straightforward coding and debugging. Sometimes you have to scrap misleading theories because you fail to reject the null hypothesis, sometimes you learn something new halfway through. However, it can be rewarding when you get results.
Data science is super important in today's world. Whether it's moneyball in sports or financial records or plague predictions, data science gives us a way to model the world around us.
P.S. remember to wash your hands.